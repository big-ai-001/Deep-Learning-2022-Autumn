 # BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (6人)

> ## 在你想報的部分標上你的名子 如:（roger 林榮顯）
> 參考資訊：  
> [台大資訊 深度學習之應用 | ADL 10.1 BERT 進擊的芝麻街巨人](https://www.youtube.com/watch?v=XS44fSQP0-E&list=PLOAQYZPRn2V5yumEV1Wa4JvRiDluf83vn&index=34)  
> [BERT 论文逐段精读【论文精读】](https://www.youtube.com/watch?v=ULD3uIb2MHQ)  
> [gluebenchmark](https://gluebenchmark.com/)  
> [GLUE基准数据集介绍及下载](https://zhuanlan.zhihu.com/p/135283598)  
> [The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)  
> [SQuAD —— text span答案类型开创者](https://zhuanlan.zhihu.com/p/137828922)

## Abstract + 1 Introduction 許禾諭

as the title

## 2 Related Work - 3.Model Architecture 謝永盛

Unsupervised Feature-based Approaches(word embedding, ELMo), Unsupervised Fine-tuning Approaches(GPT)

## 3.Input/Output Representations - 3.2 Fine-tuning BERT (暫定:Yun 林揚展, 如果有人想講這段請找我)

explain MLM, NSP, How to Fine-tuning.

## 4 Experiments - 4.3 SQuAD v2.0 

GLUE, task explain, How it train?, Model performes.

SQuAD, task explain, How it train?, Model performes.

## 4.4 SWAG - 5.1 Effect of Pre-training Tasks 

SWAG, task explain, How it train?, Model performes.

Effect of Pre-training Tasks: 表5說明, 移除NSP造成的性能下降，雙向(無向)改單向造成的性能下降.

## 5.2 Effect of Model Size - 6 Conclusion (Ken 高士凱)

use table6 explain The effect of changes in Model Size on Model performes.

use table7 Verify that BERT can do Feature extracted.

