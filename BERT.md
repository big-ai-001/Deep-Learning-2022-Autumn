 # BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (6人)

> ## 在你想報的部分標上你的名子 如:（roger 林榮顯）

## Abstract + 1 Introduction

as the title

## 2 Related Work - 3.Model Architecture

Unsupervised Feature-based Approaches(word embedding, ELMo), Unsupervised Fine-tuning Approaches(GPT)

## 3.Input/Output Representations - 3.2 Fine-tuning BERT (暫定:Yun 林揚展, 如果有人想講這段請找我)

explain MLM, NSP, How to Fine-tuning.

## 4 Experiments - 4.3 SQuAD v2.0 

GLUE, task explain, How it train?, Model performes.

SQuAD, task explain, How it train?, Model performes.

## 4.4 SWAG - 5.1 Effect of Pre-training Tasks 

SWAG, task explain, How it train?, Model performes.

Effect of Pre-training Tasks: 表5說明, 移除NSP造成的性能下降，雙向(無向)改單向造成的性能下降.

## 5.2 Effect of Model Size - 6 Conclusion

use table6 explain The effect of changes in Model Size on Model performes.

use table7 Verify that BERT can do Feature extracted.

